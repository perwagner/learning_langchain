{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.youtube.com/watch?v=ny215UUXbhI&list=PLOrU905yPYXLwua9Ci5rXKA8iRz61bpg7&index=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_1(input_1):\n",
    "    return input_1 + \" first function\"\n",
    "\n",
    "\n",
    "def function_2(input_2):\n",
    "    return input_2 + \" second function\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import Graph\n",
    "\n",
    "workflow = Graph()\n",
    "\n",
    "workflow.add_node(\"node_1\", function_1)\n",
    "workflow.add_node(\"node_2\", function_2)\n",
    "\n",
    "workflow.add_edge(\"node_1\", \"node_2\")\n",
    "\n",
    "workflow.set_entry_point(\"node_1\")\n",
    "workflow.set_finish_point(\"node_2\")\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, world! first function second function'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app.invoke(\"Hello, world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name of node: node_1\n",
      "values:  I am moving from first function\n",
      "\n",
      "\n",
      "\n",
      "Name of node: node_2\n",
      "values:  I am moving from first function second function\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input  = \"I am moving from\"\n",
    "\n",
    "for output in app.stream(input):\n",
    "    for k, v in output.items():\n",
    "        print(f\"Name of node: {k}\")\n",
    "        print(\"values: \", v)\n",
    "\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If api_key is not passed, default behavior is to use the `MISTRAL_API_KEY` environment variable.\n",
    "llm = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The amount of data needed to train a model can vary depending on the complexity of the task, the size of the model, and the quality of the data. In general, more data is better for training a model as it allows for better generalization and performance. However, there is no fixed amount of data that is considered sufficient for training a model. It is recommended to have at least thousands of data points for simple tasks, and tens of thousands to millions of data points for more complex tasks like deep learning models. It is also important to have a diverse and representative dataset to ensure that the model can generalize well to unseen data.', response_metadata={'token_usage': {'completion_tokens': 127, 'prompt_tokens': 18, 'total_tokens': 145}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-af72fefd-29da-4a37-80df-4e42dbc9e450-0', usage_metadata={'input_tokens': 18, 'output_tokens': 127, 'total_tokens': 145})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"How much data do you need to train a model?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_1(input_1):\n",
    "    complete_query = \"Your task is to provide only the topic based on the user query\" \\\n",
    "    \"Only output the topic amont: [Japan, Sports]. Don't include reasoning. Following is \" \\\n",
    "    \"the user query: \" + input_1\n",
    "\n",
    "    response = llm.invoke(complete_query)\n",
    "    return response.content\n",
    "\n",
    "\n",
    "def function_2(input_2):\n",
    "    TOPIC_UPPER = input_2.upper()\n",
    "    response = f\"Here is the topic in UPPER case: {TOPIC_UPPER}\"\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import Graph\n",
    "\n",
    "workflow = Graph()\n",
    "\n",
    "workflow.add_node(\"agent\", function_1)\n",
    "workflow.add_node(\"tool\", function_2)\n",
    "\n",
    "workflow.add_edge(\"agent\", \"tool\")\n",
    "\n",
    "workflow.set_entry_point(\"agent\")\n",
    "workflow.set_finish_point(\"tool\")\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here is the topic in UPPER case: JAPAN'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Tell me about Japan's early history\"\n",
    "app.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 with agent state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "AgentState = dict()\n",
    "\n",
    "AgentState[\"messages\"] = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tavily import TavilyClient\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "@tool\n",
    "def search(query: str) -> str:\n",
    "    \"\"\"Searches the internet for something about the query.\"\"\"\n",
    "    tavily_client = TavilyClient()\n",
    "    r = tavily_client.search(query, max_results=1)\n",
    "    answer = r.get(\"results\")[0].get(\"content\")\n",
    "    return {\"context\": answer, \"question\": query}\n",
    "\n",
    "\n",
    "# r = search(\"Japan's early history\")\n",
    "# print(r)\n",
    "# print(r.get(\"results\")[0].get(\"content\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_1(state):\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[-1]\n",
    "\n",
    "    complete_query = \"Your task is to provide only the topic based on the user query\" \\\n",
    "    \"Only output the topic amont: [Japan, Sports]. Don't include reasoning. Following is \" \\\n",
    "    \"the user query: \" + question\n",
    "\n",
    "    response = llm.invoke(complete_query)\n",
    "    state[\"messages\"].append(response.content)\n",
    "    return state\n",
    "\n",
    "\n",
    "def function_2(state):\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[0]\n",
    "\n",
    "    template = \"\"\"Create 3 bullet points answering the question based on on the following context:\n",
    "    {context}\n",
    "\n",
    "    question: {question}    \n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    retrieval_chain = (\n",
    "        search\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    result = retrieval_chain.invoke(question)\n",
    "    return result\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name of node: agent\n",
      "Answer:\n",
      "{'messages': [\"Tell me about Japan's early history\", 'Japan']}\n",
      "\n",
      "\n",
      "\n",
      "Name of node: tool\n",
      "Answer:\n",
      "- Japan's early history is marked by periods such as the Jomon, Yayoi, Kofun, and Asuka periods, each contributing to the cultural development of the nation.\n",
      "- The influence of these early periods can still be seen in traditional arts, architecture, religious practices, and societal norms in Japan today.\n",
      "- Studying Japan's early history can provide insights into the foundation of the nation's identity and help us understand how certain aspects of Japanese culture have evolved over time.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "workflow = Graph()\n",
    "\n",
    "workflow.add_node(\"agent\", function_1)\n",
    "workflow.add_node(\"tool\", function_2)\n",
    "\n",
    "workflow.add_edge(\"agent\", \"tool\")\n",
    "\n",
    "workflow.set_entry_point(\"agent\")\n",
    "workflow.set_finish_point(\"tool\")\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "input = {\"messages\": [\"Tell me about Japan's early history\"]}\n",
    "\n",
    "# app.invoke(input)\n",
    "\n",
    "\n",
    "for output in app.stream(input):\n",
    "    for k, v in output.items():\n",
    "        print(f\"Name of node: {k}\")\n",
    "        print(\"Answer:\")\n",
    "        print(v)\n",
    "\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, Sequence\n",
    "import operator\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "class TopicSelectionParser(BaseModel):\n",
    "    Topic: str = Field(description=\"Selected Topic\")\n",
    "\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=TopicSelectionParser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def search(query: str) -> str:\n",
    "    \"\"\"Searches the internet for something about the query.\"\"\"\n",
    "    tavily_client = TavilyClient()\n",
    "    r = tavily_client.search(query, max_results=1)\n",
    "    answer = r.get(\"results\")[0].get(\"content\")\n",
    "    return {\"context\": answer, \"question\": query}\n",
    "    # return answer\n",
    "\n",
    "\n",
    "def function_1(state):\n",
    "    print(\"--> calling agent...\")\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[-1]\n",
    "\n",
    "    template = \"\"\"Your task is to provide only the topic based on the user query \n",
    "    Only output the topic amont: [Japan, Sports, Not Relevant]. Don't include reasoning. Following is the user query: {question}\\n\n",
    "    {format_instructions}\"\"\"\n",
    "\n",
    "    prompt = PromptTemplate.from_template(\n",
    "        template,\n",
    "        # input_variables=[\"question\"],\n",
    "        # partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "    )\n",
    "    prompt.format(question=question, format_instructions=parser.get_format_instructions())\n",
    "\n",
    "    chain = prompt | llm | parser\n",
    "    response = chain.invoke({\n",
    "        \"question\": question,\n",
    "        \"format_instructions\": parser.get_format_instructions(),\n",
    "    })\n",
    "    print(response.Topic)\n",
    "    return {\"messages\": [response.Topic]}\n",
    "\n",
    "\n",
    "def function_2(state):\n",
    "    print(\"--> calling search tool and make bullets...\")\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[0]\n",
    "\n",
    "    template = \"\"\"Create 3 bullet points answering the question based on on the following context:\n",
    "    {context}\n",
    "\n",
    "    question: {question}    \n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "    retrieval_chain = (\n",
    "        search\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    result = retrieval_chain.invoke(question)\n",
    "    return {\"messages\": [result]}\n",
    "\n",
    "\n",
    "def function_3(state):\n",
    "    print(\"--> calling search tool and make long text...\")\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[0]\n",
    "\n",
    "    template = \"\"\"Create a short sentence answering the question based on on the following context:\n",
    "    {context}\n",
    "\n",
    "    question: {question}    \n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    retrieval_chain = (\n",
    "        search\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    result = retrieval_chain.invoke(question)\n",
    "    return {\"messages\": [result]}\n",
    "\n",
    "\n",
    "def router(state):\n",
    "    print(\"--> routing...\")\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    print(last_message)\n",
    "    if \"Japan\" in last_message or \"Sports\" in last_message:\n",
    "        return \"bullet_points\"\n",
    "    else:\n",
    "        return \"short_text\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "graph = StateGraph(AgentState)\n",
    "\n",
    "graph.add_node(\"agent\", function_1)\n",
    "graph.add_node(\"tool_with_bullets\", function_2)\n",
    "graph.add_node(\"tool_with_short_text\", function_3)\n",
    "graph.set_entry_point(\"agent\")\n",
    "\n",
    "graph.add_conditional_edges(\n",
    "    source=\"agent\",\n",
    "    path=router,\n",
    "    path_map={\n",
    "        \"bullet_points\": \"tool_with_bullets\",\n",
    "        \"short_text\": \"tool_with_short_text\",\n",
    "    },\n",
    ")\n",
    "\n",
    "graph.add_edge(\"tool_with_bullets\", END)\n",
    "graph.add_edge(\"tool_with_short_text\", END)\n",
    "\n",
    "app = graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> calling agent...\n",
      "Japan\n",
      "--> routing...\n",
      "Japan\n",
      "Name of node: agent\n",
      "Answer:\n",
      "Japan\n",
      "\n",
      "\n",
      "\n",
      "--> calling search tool and make bullets...\n",
      "Name of node: tool_with_bullets\n",
      "Answer:\n",
      "- The Jomon Period is considered the first historical period of Japan, spanning from around 14,500 to 300 BCE.\n",
      "- The name \"Jomon\" comes from the distinctive pottery produced during this time, which featured simple rope-like decoration.\n",
      "- The Jomon Period is known for its hunter-gatherer lifestyle and early agricultural practices, laying the foundation for Japan's future development.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input = {\"messages\": [\"Tell me about Japan's early history\"]}\n",
    "\n",
    "for output in app.stream(input):\n",
    "    for k, v in output.items():\n",
    "        print(f\"Name of node: {k}\")\n",
    "        print(\"Answer:\")\n",
    "        print(v.get(\"messages\")[0])\n",
    "\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> calling agent...\n",
      "Not Relevant\n",
      "--> routing...\n",
      "Not Relevant\n",
      "Name of node: agent\n",
      "Answer:\n",
      "Not Relevant\n",
      "\n",
      "\n",
      "\n",
      "--> calling search tool and make long text...\n",
      "Name of node: tool_with_short_text\n",
      "Answer:\n",
      "Denmark is a Nordic country with the oldest state flag in the world still in use by an independent nation.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input = {\"messages\": [\"Tell me a bit about Denmark\"]}\n",
    "\n",
    "for output in app.stream(input):\n",
    "    for k, v in output.items():\n",
    "        print(f\"Name of node: {k}\")\n",
    "        print(\"Answer:\")\n",
    "        print(v.get(\"messages\")[0])\n",
    "\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.youtube.com/watch?v=UVqDDM5CWYQ&list=PLOrU905yPYXLwua9Ci5rXKA8iRz61bpg7&index=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(temperature=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 8, 'total_tokens': 17}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-b14ab4fb-426d-49ba-941b-0ab3b6e99cbf-0', usage_metadata={'input_tokens': 8, 'output_tokens': 9, 'total_tokens': 17})"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"Hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain_core.messages import ToolMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.utils.function_calling import convert_to_openai_tool\n",
    "from pprint import pprint\n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Mulitplies two numbers and returns the result.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "model_with_tools = model.bind(tools=[convert_to_openai_tool(multiply)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='' additional_kwargs={'tool_calls': [{'id': 'call_tZNxvA7W962gJsJyhf4VJD3Q', 'function': {'arguments': '{\"a\":44,\"b\":11}', 'name': 'multiply'}, 'type': 'function'}]} response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 59, 'total_tokens': 76}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None} id='run-01dda886-d8b7-40c7-bf37-7faecccfe817-0' tool_calls=[{'name': 'multiply', 'args': {'a': 44, 'b': 11}, 'id': 'call_tZNxvA7W962gJsJyhf4VJD3Q', 'type': 'tool_call'}] usage_metadata={'input_tokens': 59, 'output_tokens': 17, 'total_tokens': 76}\n"
     ]
    }
   ],
   "source": [
    "r = model_with_tools.invoke(\"What is 44 * 11?\")\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'function': {'arguments': '{\"a\":44,\"b\":11}', 'name': 'multiply'},\n",
      "  'id': 'call_tZNxvA7W962gJsJyhf4VJD3Q',\n",
      "  'type': 'function'}]\n"
     ]
    }
   ],
   "source": [
    "tool_calls = r.additional_kwargs.get(\"tool_calls\")\n",
    "pprint(tool_calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function name: multiply\n",
      "Function arguments: {\"a\":44,\"b\":11}\n",
      "{'id': 'call_tZNxvA7W962gJsJyhf4VJD3Q', 'function': {'arguments': '{\"a\":44,\"b\":11}', 'name': 'multiply'}, 'type': 'function'}\n"
     ]
    }
   ],
   "source": [
    "for tool_call in tool_calls:\n",
    "    print(f\"Function name: {tool_call.get(\"function\").get(\"name\")}\")\n",
    "    print(f\"Function arguments: {tool_call.get(\"function\").get(\"arguments\")}\")\n",
    "    print(tool_call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, Sequence\n",
    "import operator\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> calling model...\n",
      "--> routing...\n",
      "--> calling tool...\n",
      "{'messages': ['What is 44 * 11?', AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_xFqggSMuzvCjQfBrikCqUT3e', 'function': {'arguments': '{\"a\":44,\"b\":11}', 'name': 'multiply'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 59, 'total_tokens': 76}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-99b8db48-e879-4a2d-ab42-f277c1b64a9e-0', tool_calls=[{'name': 'multiply', 'args': {'a': 44, 'b': 11}, 'id': 'call_xFqggSMuzvCjQfBrikCqUT3e', 'type': 'tool_call'}], usage_metadata={'input_tokens': 59, 'output_tokens': 17, 'total_tokens': 76}), 484]}\n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "graph = StateGraph(AgentState)\n",
    "\n",
    "\n",
    "def invoke_model(state):\n",
    "    print(\"--> calling model...\")\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[-1]\n",
    "    return {\"messages\": [model_with_tools.invoke(question)]}\n",
    "\n",
    "\n",
    "def invoke_tool(state):\n",
    "    print(\"--> calling tool...\")\n",
    "    tool_calls = state[\"messages\"][-1].additional_kwargs.get(\"tool_calls\", [])\n",
    "    multiply_call = None\n",
    "\n",
    "    for tool_call in tool_calls:\n",
    "        tool_name = tool_call.get(\"function\").get(\"name\")\n",
    "        if tool_name == \"multiply\":\n",
    "            multiply_call = tool_call\n",
    "\n",
    "    if multiply_call is None:\n",
    "        raise Exception(\"No multiply call found.\")\n",
    "    \n",
    "    r = multiply.invoke(json.loads(multiply_call.get(\"function\").get(\"arguments\")))\n",
    "    return {\"messages\": [r]}\n",
    "\n",
    "\n",
    "def router(state):\n",
    "    print(\"--> routing...\")\n",
    "    tool_calls = state[\"messages\"][-1].additional_kwargs.get(\"tool_calls\", [])\n",
    "    if len(tool_calls) > 0:\n",
    "        return \"multiply\"\n",
    "    else:\n",
    "        return \"end\"\n",
    "    \n",
    "\n",
    "\n",
    "graph.add_node(\"agent\", invoke_model)\n",
    "graph.add_node(\"tool\", invoke_tool)\n",
    "graph.add_edge(\"tool\", END)\n",
    "graph.set_entry_point(\"agent\")\n",
    "graph.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    router,\n",
    "    {\n",
    "        \"multiply\": \"tool\",\n",
    "        \"end\": END,\n",
    "    },\n",
    ")\n",
    "\n",
    "app = graph.compile()\n",
    "\n",
    "output = app.invoke({\"messages\": [\"What is 44 * 11?\"]})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> calling model...\n",
      "--> routing...\n",
      "{'messages': ['How do you describe a honey bee?', AIMessage(content='A honey bee is a small flying insect known for its role in pollination and honey production. Honey bees are social insects that live in colonies with a queen, worker bees, and drones. They have a yellow and black striped body, wings, and a stinger. Honey bees are essential for the pollination of plants and crops, making them crucial for agriculture and ecosystem health.', response_metadata={'token_usage': {'completion_tokens': 77, 'prompt_tokens': 59, 'total_tokens': 136}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-9e9a07fb-22f4-4d19-8039-ff49e5b00559-0', usage_metadata={'input_tokens': 59, 'output_tokens': 77, 'total_tokens': 136})]}\n"
     ]
    }
   ],
   "source": [
    "output = app.invoke({\"messages\": [\"How do you describe a honey bee?\"]})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Agent Supervisor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "llm = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent Function\n",
    "def create_agent(llm: ChatOpenAI, tools: list, system_prompt: str):\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system_prompt),\n",
    "            MessagesPlaceholder(variable_name=\"messages\"),\n",
    "            MessagesPlaceholder(variable_name=\"messages\"),\n",
    "\n",
    "        ]\n",
    "    )\n",
    "    agent = create_openai_tools_agent(llm, tools, prompt)\n",
    "    executor = AgentExecutor(agent=agent, tools=tools)\n",
    "    return executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tools\n",
    "from typing import TypedDict, Annotated, Sequence, List, Tuple, Union\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.tools import tool\n",
    "from langchain_experimental.tools import PythonREPLTool\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "tavily_tool = TavilySearchResults(max_results=2)\n",
    "python_repl_tool = PythonREPLTool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent_node(state, agent, name):\n",
    "    result = agent.invoke(state)\n",
    "    return {\"messages\": [HumanMessage(content=result[\"output\"], name=name)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "members = [\"RAG\", \"Researcher\", \"Coder\"]\n",
    "system_prompt = (\n",
    "    \"You are a supervisor tasked with managing a conversation between the \"\n",
    "    \"following members: {members}. Given the following user request, \"\n",
    "    \"respond with the worker to act next. Use RAG tool when questions are \"\n",
    "    \"related to Japan. Each worker wil perform a task and respond with their \"\n",
    "    \"results and status. When finished, respond with FINISH.\"\n",
    ")\n",
    "\n",
    "options = [\"FINISH\"] + members\n",
    "\n",
    "function_def = {\n",
    "    \"name\": \"route\",\n",
    "    \"description\": \"Select the next role.\",\n",
    "    \"parameters\": {\n",
    "        \"title\": \"routeSchema\",\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"next\": {\n",
    "                \"title\": \"Next\",\n",
    "                \"anyOf\": [\n",
    "                    {\"enum\": options},\n",
    "                ]\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"next\"],\n",
    "    }\n",
    "}\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Given the conversation above, who should act next? or should we FINISH? Select one of: {options}\",\n",
    "        ),\n",
    "    ]\n",
    ").partial(options=str(options), members=\", \".join(members))\n",
    "\n",
    "supervisor_chain = (\n",
    "    prompt\n",
    "    | llm.bind_functions(functions=[function_def], function_call=\"route\")\n",
    "    | JsonOutputFunctionsParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Annotated, Any, Dict, List, Optional, Sequence, TypedDict\n",
    "import functools\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langgraph.graph import StateGraph, END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "    next: str\n",
    "\n",
    "\n",
    "research_agent = create_agent(llm, [tavily_tool], \"You are a web researcher\")\n",
    "research_node = functools.partial(agent_node, agent=research_agent, name=\"Researcher\")\n",
    "\n",
    "code_agent = create_agent(\n",
    "    llm,\n",
    "    [python_repl_tool],\n",
    "    \"You are a coder. You will write code to solve the problem.\"\n",
    ")\n",
    "code_node = functools.partial(agent_node, agent=code_agent, name=\"Coder\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
