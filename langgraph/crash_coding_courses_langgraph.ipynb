{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nodes and edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langgraph.graph import END, MessageGraph\n",
    "\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_one(input: list[HumanMessage]):\n",
    "    input[0].content = input[0].content + \"a\"\n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = MessageGraph()\n",
    "\n",
    "graph.add_node(\"branch_a\", add_one)\n",
    "graph.add_node(\"branch_b\", add_one)\n",
    "graph.add_node(\"branch_c\", add_one)\n",
    "graph.add_node(\"final_node\", add_one)\n",
    "\n",
    "graph.add_edge(\"branch_a\", \"branch_b\")\n",
    "graph.add_edge(\"branch_a\", \"branch_c\")\n",
    "graph.add_edge(\"branch_b\", \"final_node\")\n",
    "graph.add_edge(\"branch_c\", \"final_node\")\n",
    "graph.add_edge(\"final_node\", END)\n",
    "\n",
    "graph.set_entry_point(\"branch_a\")\n",
    "\n",
    "runnable = graph.compile()\n",
    "\n",
    "Image(runnable.get_graph().draw_mermaid_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runnable.invoke(\"a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entry(input: list[HumanMessage]):\n",
    "    return input\n",
    "\n",
    "\n",
    "def work_with_b(input: list[HumanMessage]):\n",
    "    print(\"use_b\")\n",
    "    return input\n",
    "\n",
    "\n",
    "def work_with_c(input: list[HumanMessage]):\n",
    "    print(\"use_c\")\n",
    "    return input\n",
    "\n",
    "\n",
    "def router(input: list[HumanMessage]):\n",
    "    if \"use_b\" in input[0].content:\n",
    "        return \"use_b\"\n",
    "    else:\n",
    "        return \"use_c\"\n",
    "    \n",
    "\n",
    "graph = MessageGraph()\n",
    "\n",
    "graph.add_node(\"entry\", entry)\n",
    "graph.add_node(\"branch_b\", work_with_b)\n",
    "graph.add_node(\"branch_c\", work_with_c)\n",
    "\n",
    "graph.add_conditional_edges(\n",
    "    \"entry\",\n",
    "    router,\n",
    "    {\n",
    "        \"use_b\": \"branch_b\",\n",
    "        \"use_c\": \"branch_c\"\n",
    "    },\n",
    ")\n",
    "graph.add_edge(\"branch_b\", END)\n",
    "graph.add_edge(\"branch_c\", END)\n",
    "\n",
    "graph.set_entry_point(\"entry\")\n",
    "\n",
    "runnable = graph.compile()\n",
    "\n",
    "Image(runnable.get_graph().draw_mermaid_png())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runnable.invoke(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runnable.invoke(\"use_b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cycles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langgraph.graph import END, MessageGraph\n",
    "\n",
    "llm = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entry(input: list[HumanMessage]):\n",
    "    return input\n",
    "\n",
    "\n",
    "def action(input: list[HumanMessage]):\n",
    "    print(\"Action Taken: \", [msg.content for msg in input])\n",
    "\n",
    "    if len(input) > 5:\n",
    "        input.append(HumanMessage(content=\"end\"))\n",
    "    else:\n",
    "        input.append(HumanMessage(content=\"continue\"))\n",
    "\n",
    "    return input\n",
    "\n",
    "\n",
    "def should_continue(input: list):\n",
    "    last_message = input[-1]\n",
    "    if \"end\" in last_message.content:\n",
    "        return \"__end__\"\n",
    "    else:\n",
    "        return \"action\"\n",
    "    \n",
    "\n",
    "graph = MessageGraph()\n",
    "\n",
    "graph.add_node(\"agent\", entry)\n",
    "graph.add_node(\"action\", action)\n",
    "\n",
    "graph.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"action\": \"action\",\n",
    "        \"__end__\": END\n",
    "    },\n",
    ")\n",
    "graph.add_edge(\"action\", \"agent\")\n",
    "graph.set_entry_point(\"agent\")\n",
    "\n",
    "runnable = graph.compile()\n",
    "\n",
    "Image(runnable.get_graph().draw_mermaid_png())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runnable.invoke(\"Hello world\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "import random\n",
    "from typing import Annotated, Sequence, TypedDict\n",
    "\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, ToolMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import END, MessageGraph\n",
    "\n",
    "llm = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "    api_call_count: int = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def fake_weather_api(city: str) -> str:\n",
    "    \"\"\"Returns the weather for a given city.\"\"\"\n",
    "    if random.randint(0, 3) == 0:\n",
    "        return f\"Sunny, {random.randint(10, 30)} degrees\"\n",
    "    else:\n",
    "        return \"Service unavailable\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools = llm.bind_tools([fake_weather_api])\n",
    "\n",
    "tool_mapping = {\n",
    "    \"fake_weather_api\": fake_weather_api,\n",
    "}\n",
    "\n",
    "messages = [HumanMessage(\"How will the weather be in Copenhagen today? I would like to eat outside if possible\")]\n",
    "llm_output = llm_with_tools.invoke(messages)\n",
    "messages.append(llm_output)\n",
    "\n",
    "\n",
    "for tool_call in llm_output.tool_calls:\n",
    "    tool_name = tool_call[\"name\"].lower()\n",
    "    tool = tool_mapping[tool_name]\n",
    "    tool_output = tool.invoke(tool_call[\"args\"])\n",
    "    messages.append(ToolMessage(content=tool_output, tool_call_id=tool_call[\"id\"]))\n",
    "\n",
    "llm_with_tools.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_continue(state: AgentState):\n",
    "    print(\"--> should_continue\")\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    print(f\"State: {state}\")\n",
    "    print(f\"Last message: {last_message}\")\n",
    "    print(f\"Tool calls: {last_message.tool_calls}\")\n",
    "\n",
    "    if not last_message.tool_calls:\n",
    "        print(\"---> Going to end\")\n",
    "        return \"end\"\n",
    "    else:\n",
    "        print(\"---> Going to continue\")\n",
    "        return \"continue\"\n",
    "    \n",
    "\n",
    "def call_model(state: AgentState):\n",
    "    print(\"--> call_model\")\n",
    "    messages = state[\"messages\"]\n",
    "    response = llm_with_tools.invoke(messages)\n",
    "\n",
    "    print(\"Response: \", response)\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"messages\": [response],\n",
    "        \"api_call_count\": state[\"api_call_count\"],\n",
    "    }\n",
    "\n",
    "\n",
    "def call_tool(state: AgentState):\n",
    "    print(\"--> call_tool\")\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    tool_call = last_message.tool_calls[0]\n",
    "    tool_name = tool_call[\"name\"].lower()\n",
    "    tool = tool_mapping[tool_name]\n",
    "    tool_output = tool.invoke(tool_call[\"args\"])\n",
    "\n",
    "    state[\"api_call_count\"] += 1\n",
    "    print(\"Tool output: \", tool_output)\n",
    "    print(\"API call count after this tool call: \", state[\"api_call_count\"])\n",
    "    tool_message = ToolMessage(content=tool_output, tool_call_id=tool_call[\"id\"])\n",
    "    return {\n",
    "        \"messages\": [tool_message],\n",
    "        \"api_call_count\": state[\"api_call_count\"],\n",
    "    }\n",
    "\n",
    "\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_node(\"action\", call_tool)\n",
    "\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"continue\": \"action\",\n",
    "        \"end\": END\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"action\", \"agent\")\n",
    "workflow.set_entry_point(\"agent\")\n",
    "\n",
    "app = workflow.compile()\n",
    "Image(app.get_graph().draw_mermaid_png())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "system_message = SystemMessage(content=\"You are responsible for answering user questions. You use tools for that, These tools sometimes fail and you are very resilient and trying them again\")\n",
    "human_message = HumanMessage(content=\"How will the weather be in Copenhagen today?\")\n",
    "messages = [system_message, human_message]\n",
    "\n",
    "r = app.invoke({\"messages\": messages, \"api_call_count\": 0})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangGraph vs LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embedding_function = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "    Document(page_content=\"Raistlin is a mage\", metadata={\"source\": \"Dragonlance\"}),\n",
    "    Document(page_content=\"Tanis is a ranger\", metadata={\"source\": \"Dragonlance\"}),\n",
    "]\n",
    "\n",
    "\n",
    "db = Chroma.from_documents(docs, embedding_function)\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "retrieval_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_chain.invoke(\"What type is Tanis?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With LangGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "\n",
    "chain_with_prompt = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    question: str\n",
    "    raw_docs: list[BaseMessage]\n",
    "    formatted_docs: list[str]\n",
    "    generation: str\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_docs(state: AgentState):\n",
    "    print(\"Getting documents...\", state)\n",
    "    question = state[\"question\"]\n",
    "    docs = retriever.invoke(question)\n",
    "    state[\"raw_docs\"] = docs\n",
    "    return state\n",
    "\n",
    "\n",
    "def format_docs(state: AgentState):\n",
    "    print(\"Formatting documents...\", state)\n",
    "    documents = state[\"raw_docs\"]\n",
    "    state[\"formatted_docs\"] = \"\\n\\n\".join([doc.page_content for doc in documents])\n",
    "    return state\n",
    "\n",
    "\n",
    "def generate(state: AgentState):\n",
    "    print(\"Generating response...\", state)\n",
    "    question = state[\"question\"]\n",
    "    formatted_docs = state[\"formatted_docs\"]\n",
    "    result = chain_with_prompt.invoke({\"question\": question, \"context\": formatted_docs})\n",
    "    state[\"generation\"] = result\n",
    "    return state\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from IPython.display import Image, display\n",
    "\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "workflow.add_node(\"get_docs\", get_docs)\n",
    "workflow.add_node(\"format_docs\", format_docs)\n",
    "workflow.add_node(\"generate\", generate)\n",
    "\n",
    "workflow.add_edge(\"get_docs\", \"format_docs\")\n",
    "workflow.add_edge(\"format_docs\", \"generate\")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "\n",
    "workflow.set_entry_point(\"get_docs\")\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "display(Image(app.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.invoke({\"question\": \"What type is Tanis?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Agents - Boss and subordinate agents\n",
    "[langgraph/crash_coding_courses_langgraph.ipynb (2-2)](https://www.youtube.com/watch?v=9HhcFiSgLok&list=PLNVqeXDm5tIqUIPQHLk5Xw5mpisruvsac&index=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransferNewsGrader(BaseModel):\n",
    "    \"\"\"Binary score for relevance check on football transfer news\"\"\"\n",
    "    binary_score: str = Field(description=\"The article is about football transfers, 'yes' og 'no'\")\n",
    "\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "structured_llm_grader = llm.with_structured_output(TransferNewsGrader)\n",
    "\n",
    "system = \"\"\"You are a grader assessing whether a news article conserns a football transfer. \\n\n",
    "Check if the article explicitly mentions player transfers between clubs, potential transfers or confirmed transfers. \\n\n",
    "If the article is about football transfers, answer 'yes', otherwise answer 'no'.\"\"\"\n",
    "\n",
    "grade_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system),\n",
    "    (\"human\", \"News Article:\\n\\n {article}\"),\n",
    "])\n",
    "\n",
    "evaluator = grade_prompt | structured_llm_grader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = evaluator.invoke({\"article\": \"Raistlin is a powerfull mage\"})\n",
    "print(r)\n",
    "print(r.binary_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArticlePostabilityGrader(BaseModel):\n",
    "    \"\"\"Binary check if article is postable given word_count, sensationalism and language\"\"\"\n",
    "    can_be_posted: str = Field(description=\"The article can be posted (meets_word_count, is_sensational, is_in_danish_language are all 'yes'), 'yes' or 'no'\")\n",
    "    meets_word_count: str = Field(description=\"The article has more than 200 words, 'yes' or 'no'\")\n",
    "    is_sensational: str = Field(description=\"The article is written in a sensational style, 'yes' or 'no'\")\n",
    "    is_in_danish_language: str = Field(description=\"The article is written in Danish, 'yes' or 'no'\")\n",
    "\n",
    "llm_postability = ChatOpenAI()\n",
    "structured_llm_postability_grader = llm_postability.with_structured_output(ArticlePostabilityGrader)\n",
    "\n",
    "postability_system = \"\"\"You are a grader assessing whether a news article is ready to be posted, if it meets the minimum word count of 200 words, is written in a sensationalistic style, and if it is in Danish. \\n\n",
    "    Evaluate the article for grammatical errors, completeness, appropriateness for publication, and EXAGERATED sensationalism. \\n\n",
    "    Also, confirm if the language used in the article is Danish and it meets the word count requirement. \\n\n",
    "    Provide four binary scores: one to indicate if the article can be posted ('yes' or 'no'), one for adequate word count ('yes' or 'no'), one for sensationalistic writing ('yes' or 'no'), and another if the language is Danish ('yes' or 'no').\n",
    "\"\"\"\n",
    "postability_grade_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", postability_system),\n",
    "    (\"human\", \"News Article:\\n\\n {article}\"),\n",
    "])\n",
    "news_chef = postability_grade_prompt | structured_llm_postability_grader\n",
    "\n",
    "# result = news_chef.invoke({\"article\": \"Alex Keaton is transfering to the mexican club Chivas to be closer to his pornstar girlfriend\"})\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_translation = ChatOpenAI()\n",
    "\n",
    "translation_system = \"\"\"You are a translator translating an article into Danish. \\n\n",
    "Translate the article accurately while mainting the original meaning.\"\"\"\n",
    "translation_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", translation_system),\n",
    "    (\"human\", \"English text:\\n\\n {article}\"),\n",
    "])\n",
    "\n",
    "translator = translation_prompt | llm_translation\n",
    "# result = translator.invoke({\"article\": \"Alex Keaton is transfering to the mexican club Chivas to be closer to his pornstar girlfriend\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_expansion = ChatOpenAI()\n",
    "expansion_system = \"\"\"You are a writer tasked with expanding the given article to at least 200 words while maintaining the original meaning of the article\"\"\"\n",
    "expansion_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", expansion_system),\n",
    "    (\"human\", \"News Article:\\n\\n {article}\"),\n",
    "])\n",
    "\n",
    "expander = expansion_prompt | llm_expansion\n",
    "# result = expander.invoke({\"article\": \"Alex Keaton is transfering to the mexican club Chivas to be closer to his pornstar girlfriend\"})\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Literal\n",
    "\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    article_state: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transfer_news_grade(state: AgentState):\n",
    "    print(f\"get_transfer_news_grade: {state}\")\n",
    "    print(\"Evaluator: Reading article but doing nothing to change it...\")\n",
    "    return state\n",
    "\n",
    "\n",
    "def evaluate_article(state: AgentState):\n",
    "    print(f\"Evaluate_article: {state}\")\n",
    "    print(\"News: Reading article but doing nothing to change it\")\n",
    "    return state\n",
    "\n",
    "\n",
    "def translate_article(state: AgentState):\n",
    "    print(f\"Translate_article: {state}\")\n",
    "    article = state[\"article_state\"]\n",
    "    result = translator.invoke({\"article\": article})\n",
    "    state[\"article_state\"] = result.content\n",
    "    return state\n",
    "\n",
    "\n",
    "def expand_article(state: AgentState):\n",
    "    print(f\"Expand_article: {state}\")\n",
    "    article = state[\"article_state\"]\n",
    "    result = expander.invoke({\"article\": article})\n",
    "    state[\"article_state\"] = result.content\n",
    "    return state\n",
    "\n",
    "\n",
    "def publisher(state: AgentState):\n",
    "    print(f\"Publisher: {state}\")\n",
    "    print(\"PUBLISHING...\")\n",
    "    return state\n",
    "\n",
    "\n",
    "def evaluator_router(state: AgentState) -> Literal[\"news_chef\", \"not_relevant\"]:\n",
    "    article = state[\"article_state\"]\n",
    "    evaluator = grade_prompt | structured_llm_grader\n",
    "    result = evaluator.invoke({\"article\": article})\n",
    "\n",
    "    print(f\"evaluator_router state: {state}\")\n",
    "    print(\"Evaluator result: \", result)\n",
    "\n",
    "    if result.binary_score == \"yes\":\n",
    "        return \"news_chef\"\n",
    "    else:\n",
    "        return \"not_relevant\"\n",
    "\n",
    "\n",
    "def news_chef_router(state: AgentState) -> Literal[\"translator\", \"publisher\", \"expander\"]:\n",
    "    article = state[\"article_state\"]\n",
    "    result = news_chef.invoke({\"article\": article})\n",
    "    print(f\"news_chef_router state: {state}\")\n",
    "    print(\"News chef result: \", result)\n",
    "\n",
    "    if result.can_be_posted == \"yes\":\n",
    "        return \"publisher\"\n",
    "    elif result.is_in_danish_language == \"no\":\n",
    "        if result.meets_word_count == \"no\" or result.is_sensational == \"no\":\n",
    "            return \"expander\"\n",
    "    \n",
    "    return \"translator\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "workflow.add_node(\"evaluator\", get_transfer_news_grade)\n",
    "workflow.add_node(\"news_chef\", evaluate_article)\n",
    "workflow.add_node(\"translator\", translate_article)\n",
    "workflow.add_node(\"expander\", expand_article)\n",
    "workflow.add_node(\"publisher\", publisher)\n",
    "\n",
    "workflow.set_entry_point(\"evaluator\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"evaluator\",\n",
    "    evaluator_router,\n",
    "    {\"news_chef\": \"news_chef\", \"not_relevant\": END}\n",
    ")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"news_chef\",\n",
    "    news_chef_router,\n",
    "    {\"translator\": \"translator\", \"publisher\": \"publisher\", \"expander\": \"expander\"}\n",
    ")\n",
    "workflow.add_edge(\"translator\", \"news_chef\")\n",
    "workflow.add_edge(\"expander\", \"news_chef\")\n",
    "workflow.add_edge(\"publisher\", END)\n",
    "\n",
    "\n",
    "app = workflow.compile()\n",
    "display(Image(app.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.invoke({\"article_state\": \"Alex Keaton is transfering to the mexican club Chivas to be closer to his pornstar girlfriend\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer support bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make use of docker compose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added customer: John Doe with id: 1\n",
      "Added food item: Pizza Margherita with price: 9\n",
      "Added food item: Pizza Pepperoni with price: 10\n",
      "Added food item: Pizza Quattro Stagioni with price: 11\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import (\n",
    "    create_engine,\n",
    "    Column,\n",
    "    Integer,\n",
    "    String,\n",
    "    ForeignKey,\n",
    "    Table,\n",
    "    DateTime,\n",
    ")\n",
    "\n",
    "from sqlalchemy.orm import relationship, sessionmaker, declarative_base\n",
    "from datetime import datetime\n",
    "\n",
    "Base = declarative_base()\n",
    "\n",
    "class Customer(Base):\n",
    "    __tablename__ = \"customers\"\n",
    "\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    name = Column(String, nullable=False)\n",
    "\n",
    "    orders = relationship(\"Order\", back_populates=\"customer\")\n",
    "\n",
    "\n",
    "class FoodItem(Base):\n",
    "    __tablename__ = \"food_items\"\n",
    "\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    name = Column(String, nullable=False)\n",
    "    price = Column(Integer, nullable=False)\n",
    "\n",
    "    orders = relationship(\"Order\", back_populates=\"food_item\")\n",
    "\n",
    "\n",
    "class Order(Base):\n",
    "    __tablename__ = \"orders\"\n",
    "\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    customer_id = Column(Integer, ForeignKey(\"customers.id\"), nullable=False)\n",
    "    food_item_id = Column(Integer, ForeignKey(\"food_items.id\"), nullable=False)\n",
    "    order_date = Column(DateTime, default=datetime.now)\n",
    "    delivery_status = Column(String, nullable=False)\n",
    "\n",
    "    customer = relationship(\"Customer\", back_populates=\"orders\")\n",
    "    food_item = relationship(\"FoodItem\", back_populates=\"orders\")\n",
    "\n",
    "\n",
    "engine = create_engine(\"postgresql+psycopg2://myuser:mypassword@localhost:5433/mydatabase\")\n",
    "Base.metadata.create_all(engine)\n",
    "\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()\n",
    "\n",
    "new_customer = Customer(name=\"John Doe\")\n",
    "session.add(new_customer)\n",
    "session.commit()\n",
    "\n",
    "added_customer = session.query(Customer).filter_by(name=\"John Doe\").first()\n",
    "print(f\"added customer: {added_customer.name} with id: {added_customer.id}\")\n",
    "\n",
    "pizza1 = FoodItem(name=\"Pizza Margherita\", price=8.5)\n",
    "pizza2 = FoodItem(name=\"Pizza Pepperoni\", price=9.5)\n",
    "pizza3 = FoodItem(name=\"Pizza Quattro Stagioni\", price=10.5)\n",
    "\n",
    "session.add_all([pizza1, pizza2, pizza3])\n",
    "session.commit()\n",
    "\n",
    "added_food_items = session.query(FoodItem).all()\n",
    "for food in added_food_items:\n",
    "    print(f\"Added food item: {food.name} with price: {food.price}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, BaseMessage\n",
    "\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    question: str\n",
    "    messages: list[BaseMessage]\n",
    "    customer_name: str\n",
    "    tool_calls: list[str]\n",
    "    order_check: dict[str, str]\n",
    "    generation: str\n",
    "    sys_msg: SystemMessage\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "system = \"\"\"You task is to identify items in the question of a User: Identify the following items:\n",
    "\n",
    "food_items (str): List of food item names. Respond with 'Yes' if the food items are provided and 'No' if they are missing.\n",
    "delivery_address (str): Delivery address for the order. Respond with 'Yes' if the delivery address is provided and 'No' if it is missing.\n",
    "order_date (str): Date and time for the order. Respond with 'Yes' if the order date is provided and 'No' if it is missing.\n",
    "Again: Remember, ONLY answer with 'YES' and 'NO' for each item.\n",
    "\n",
    "Examples:\n",
    "\"I want to order a pizza Salami\" -> 'food_items': 'Yes', 'delivery_address': 'No', 'order_date': 'No'\n",
    "\"I want to order a pizza Salami at 9pm\" -> 'food_items': 'Yes', 'delivery_address': 'No', 'order_date': 'Yes'\n",
    "\"I want to order a pizza Salami to 123 Fakestreet, Chicago\" -> 'food_items': 'Yes', 'delivery_address': 'Yes', 'order_date': 'No'\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "order_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system),\n",
    "    (\"human\", \"User: {question}\")\n",
    "])\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "order_checker_llm = order_prompt | llm | StrOutputParser()\n",
    "# order_checker_llm.invoke({\"question\": \"I want to order a pizza Salami to 123 Fakestreet, Chicago\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your information is incomplete: Please provide the date and time for the order.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_inform = \"\"\"Based on the order details provided, inform the user of any missing information.\n",
    "If the food items are missing, include \"Please specify the food items you want to order.\"\n",
    "If the delivery address is missing, include \"Please provide the delivery address.\"\n",
    "If the order date is missing, include \"Please provide the date and time for the order.\"\n",
    "\n",
    "For example, if both the delivery address and order date are missing, the message should be \"Your information is incomplete: Please provide your delivery address and order date.\"\n",
    "\"\"\"\n",
    "\n",
    "inform_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_inform),\n",
    "        (\"human\", \"{information}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "missing_info_chain = inform_prompt | llm | StrOutputParser()\n",
    "missing_info_chain.invoke(\n",
    "    {\n",
    "        \"information\": \"{'food_items': 'Yes', 'delivery_address': 'Yes', 'order_date': 'No'}\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_name_from_token(state: str):\n",
    "    return \"John Doe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "@tool\n",
    "def create_order(customer_name: str, food_items: list, delivery_address: str, order_date: str):\n",
    "    \"\"\"Create a new order for a customer with a list of food items, a delivery address and an order date.\n",
    "    \n",
    "    Args:\n",
    "        customer_name (str): The name of the customer.\n",
    "        food_items (list): A list of food items to order.\n",
    "        delivery_address (str): The delivery address for the order.\n",
    "        order_date (str): The date and time for the order.\n",
    "\n",
    "    Returns:\n",
    "        str: A string containing the details of the latest order.\n",
    "        str: Error message if the customer or any food item is not found.\n",
    "\n",
    "    This function interacts with the database to create new orders for the specified customer.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        customer = session.query(Customer).filter_by(name=customer_name).first()\n",
    "        if not customer:\n",
    "            return f\"Customer with name {customer_name} not found.\"\n",
    "\n",
    "        latest_order = None\n",
    "        order_datetime = datetime.strptime(order_date, \"%Y-%m-%d %H:%M\")\n",
    "\n",
    "        for food_name in food_items:\n",
    "            food_item = session.query(FoodItem).filter_by(name=food_name).first()\n",
    "            if not food_item:\n",
    "                return f\"Food item {food_name} not found.\"\n",
    "            new_order = Order(\n",
    "                customer_id=customer.id,\n",
    "                food_item_id=food_item.id,\n",
    "                delivery_address=delivery_address,\n",
    "                order_date=order_datetime,\n",
    "            )\n",
    "            session.add(new_order)\n",
    "            latest_order = new_order\n",
    "\n",
    "        session.commit()\n",
    "\n",
    "        # Return the latest order details as a string\n",
    "        return f\"Order placed: {customer_name} ordered {food_items} to {delivery_address} at {latest_order.order_date}\"\n",
    "    except Exception as e:\n",
    "        session.rollback()\n",
    "        return f\"Failed to execute. Error: {repr(e)}\"    \n",
    "    \n",
    "\n",
    "@tool\n",
    "def get_all_orders(customer_name: str):\n",
    "    \"\"\"\n",
    "    Retrieve all orders for a customer.\n",
    "\n",
    "    Args:\n",
    "        customer_name (str): Name of the customer whose orders are to be retrieved.\n",
    "\n",
    "    Returns:\n",
    "        str: A string containing the details of the retrieved orders.\n",
    "        str: Error message if the customer is not found or if no orders are found.\n",
    "\n",
    "    This function interacts with the database to retrieve all orders for the specified customer.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        customer = session.query(Customer).filter_by(name=customer_name).first()\n",
    "        if not customer:\n",
    "            return f\"Customer with name {customer_name} not found.\"\n",
    "\n",
    "        orders = session.query(Order).filter_by(customer_id=customer.id).all()\n",
    "\n",
    "        if not orders:\n",
    "            return f\"No orders found for customer {customer_name}.\"\n",
    "\n",
    "        order_details = []\n",
    "        for order in orders:\n",
    "            food_item = session.query(FoodItem).filter_by(id=order.food_item_id).first()\n",
    "            order_details.append(\n",
    "                f\"Order ID: {order.id}, Food Item: {food_item.name}, Price: {food_item.price}, \"\n",
    "                f\"Delivery Address: {order.delivery_address}, Order Date: {order.order_date}\"\n",
    "            )\n",
    "\n",
    "        return \"\\n\".join(order_details)\n",
    "    except Exception as e:\n",
    "        session.rollback()\n",
    "        return f\"Failed to execute. Error: {repr(e)}\"    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
